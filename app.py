# -*- coding: utf-8 -*-
# app.py: EMR AI - FINAL LAZY LOADING AND OPTIMIZATION FOR RENDER STABILITY
import os
import io
import base64
import logging
import tempfile
import numpy as np
import pandas as pd
import requests
from flask import (
    Flask, flash, redirect, render_template, request, session, url_for
)
from werkzeug.utils import secure_filename
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from PIL import Image
from functools import wraps
from ydata_profiling import ProfileReport

# === LOGGING ===
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# === FLASK SETUP ===
app = Flask(__name__)
app.secret_key = os.environ.get("FLASK_SECRET_KEY", "emr-secure-2025")
app.config['MAX_CONTENT_LENGTH'] = 10 * 1024 * 1024  # 10MB
MAX_FILE_SIZE_MB = 10
ALLOWED_IMG_EXT = {'png', 'jpg', 'jpeg', 'bmp'}
ALLOWED_DATA_EXT = {'csv', 'xls', 'xlsx'}

# === MODEL CONFIGURATION ===
MODEL_FOLDER = "models"
os.makedirs(MODEL_FOLDER, exist_ok=True)
MODEL_PATH = os.path.join(MODEL_FOLDER, "best_weights_model.keras")
HF_MODEL_URL = "https://huggingface.co/spaces/minhtriizkooooo/EMR-Analysis-Cancer-Detection/resolve/main/models/best_weights_model.keras"

# Global variable for LAZY LOADING
model = None

def get_model():
    """
    Load the Keras model just-in-time (JIT) and cache it.
    This prevents the application from crashing during startup (Error 502/500).
    """
    global model
    if model is None:
        logger.info("JIT Model loading started...")
        try:
            if not os.path.exists(MODEL_PATH):
                logger.info("Model not found locally. Downloading from HF...")
                # TƒÉng timeout t·∫£i model l√™n 5 ph√∫t (300 gi√¢y)
                r = requests.get(HF_MODEL_URL, stream=True, timeout=300) 
                r.raise_for_status()
                with open(MODEL_PATH, "wb") as f:
                    for chunk in r.iter_content(8192):
                        f.write(chunk)
                logger.info("Model downloaded successfully.")
            
            # T·∫£i model v√†o b·ªô nh·ªõ
            model = load_model(MODEL_PATH)
            logger.info("‚úÖ Keras Model loaded successfully into memory.")
        except Exception as e:
            logger.error(f"‚ùå JIT Model load failed: {e}")
            model = None
            raise RuntimeError("Failed to load AI model.") from e
            
    return model

# === UTILS (Kh√¥ng ƒë·ªïi) ===
def allowed_file(filename, exts):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in exts

def safe_thumbnail(img_bytes, size=200):
    try:
        img = Image.open(io.BytesIO(img_bytes))
        img.thumbnail((size, size), Image.Resampling.LANCZOS)
        buf = io.BytesIO()
        img.save(buf, 'JPEG', quality=85)
        return base64.b64encode(buf.getvalue()).decode()
    except Exception as e:
        logger.error(f"Thumbnail generation error: {e}")
        return None

def login_required(f):
    @wraps(f)
    def wrap(*args, **kwargs):
        if 'user' not in session: return redirect(url_for('login'))
        return f(*args, **kwargs)
    return wrap

# === ROUTES ===
@app.route("/")
def home(): 
    return redirect(url_for("login"))

@app.route("/login", methods=["GET", "POST"])
def login():
    if request.method == "POST":
        if request.form.get("userID") == "user_demo" and request.form.get("password") == "Test@123456":
            session['user'] = "user_demo"
            return redirect(url_for("dashboard"))
        flash("Sai ID ho·∫∑c m·∫≠t kh·∫©u.", "danger")
    return render_template("index.html")

@app.route("/dashboard")
@login_required
def dashboard():
    # Ki·ªÉm tra tr·∫°ng th√°i model ƒë·ªÉ hi·ªÉn th·ªã, kh√¥ng t·∫£i model ·ªü ƒë√¢y.
    status = "Model ƒê√£ S·∫µn S√†ng (Lazy Loaded)" if model else "Model Ch∆∞a T·∫£i"
    return render_template("dashboard.html", model_status=status)

@app.route("/logout")
def logout():
    session.clear()
    flash("ƒê√£ ƒëƒÉng xu·∫•t.", "info")
    return redirect(url_for("login"))

@app.route("/health")
def health():
    return {"status": "ok", "model_loaded": model is not None}, 200

# === EMR PROFILE: T·ªêI ∆ØU PANDAS & YDATA (V·∫´n c·∫ßn Gunicorn Timeout cao) ===
@app.route("/emr_profile", methods=["GET", "POST"])
@login_required
def emr_profile():
    profile_html = None
    filename = None

    if request.method == "POST":
        file = request.files.get("file")
        if not file or not file.filename:
            flash("Vui l√≤ng ch·ªçn file.", "danger")
            return render_template("emr_profile.html")

        filename = secure_filename(file.filename)
        if not allowed_file(filename, ALLOWED_DATA_EXT):
            flash("Ch·ªâ h·ªó tr·ª£ CSV, XLS, XLSX.", "danger")
            return render_template("emr_profile.html")

        try:
            file_bytes = file.read()
            if len(file_bytes) > MAX_FILE_SIZE_MB * 1024 * 1024:
                flash(f"File qu√° l·ªõn (> {MAX_FILE_SIZE_MB}MB).", "danger")
                return render_template("emr_profile.html")

            stream = io.BytesIO(file_bytes)
            
            # ƒê·ªçc d·ªØ li·ªáu v·ªõi t·ªëi ∆∞u h√≥a
            if filename.lower().endswith('.csv'):
                df = pd.read_csv(stream, low_memory=False) 
            else:
                df = pd.read_excel(stream, engine='openpyxl') 

            # === T·ªêI ∆ØU H√ìA: L·∫•y m·∫´u v√† minimal=True
            if len(df) > 2000:
                df_size = len(df)
                df = df.sample(2000, random_state=42)
                flash(f"File c√≥ {df_size} d√≤ng. ƒêang ph√¢n t√≠ch m·∫´u 2000 d√≤ng ƒë·ªÉ tr√°nh Timeout v√† Crash.", "warning")

            flash("üïí ƒêang t·∫°o b√°o c√°o chuy√™n s√¢u. Qu√° tr√¨nh n√†y **Y√äU C·∫¶U Gunicorn Timeout l·ªõn**.", "info")
            profile = ProfileReport(
                df,
                title=f"Ph√¢n t√≠ch D·ªØ li·ªáu EMR: {filename}",
                minimal=True,  # CH·∫æ ƒê·ªò NHANH V√Ä ·ªîN ƒê·ªäNH NH·∫§T
                html={"style": {"full_width": True}}
            )
            profile_html = profile.to_html()
            flash("‚úÖ B√°o c√°o chuy√™n s√¢u ho√†n th√†nh!", "success")

        except Exception as e:
            logger.error(f"Profile error: {e}")
            flash(f"‚ùå L·ªói x·ª≠ l√Ω d·ªØ li·ªáu: {str(e)}. Vui l√≤ng ki·ªÉm tra ƒë·ªãnh d·∫°ng file (header, encoding) ho·∫∑c d√πng file nh·ªè h∆°n.", "danger")

    return render_template("emr_profile.html", profile_html=profile_html, filename=filename)

# === EMR PREDICTION: S·ª¨ D·ª§NG H√ÄM get_model() ===
@app.route("/emr_prediction", methods=["GET", "POST"])
@login_required
def emr_prediction():
    prediction = None
    filename = None
    image_b64 = None

    if request.method == "POST":
        # G·ªçi h√†m t·∫£i m√¥ h√¨nh (T·∫£i l·∫ßn ƒë·∫ßu n·∫øu ch∆∞a c√≥)
        try:
            current_model = get_model()
        except RuntimeError as e:
            flash(f"‚ùå L·ªói d·ª± ƒëo√°n: {e}", "danger")
            return render_template("emr_prediction.html")

        file = request.files.get("file")
        if not file or not file.filename:
            flash("Vui l√≤ng ch·ªçn ·∫£nh.", "danger")
            return render_template("emr_prediction.html")

        filename = secure_filename(file.filename)
        if not allowed_file(filename, ALLOWED_IMG_EXT):
            flash("Ch·ªâ h·ªó tr·ª£ JPG, PNG, BMP.", "danger")
            return render_template("emr_prediction.html")

        img_bytes = file.read()
        if len(img_bytes) > MAX_FILE_SIZE_MB * 1024 * 1024:
            flash(f"·∫¢nh >{MAX_FILE_SIZE_MB}MB.", "danger")
            return render_template("emr_prediction.html")

        image_b64 = safe_thumbnail(img_bytes)

        tmp_path = None
        try:
            with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(filename)[1]) as tmp:
                tmp.write(img_bytes)
                tmp_path = tmp.name

            img = load_img(tmp_path, target_size=(240, 240))
            arr = img_to_array(img) / 255.0
            arr = np.expand_dims(arr, axis=0)

            # D·ª± ƒëo√°n b·∫±ng current_model
            prob = float(current_model.predict(arr, verbose=0)[0][0])
            result = "Nodule (C√≥ kh·ªëi u)" if prob > 0.5 else "Non-nodule (Kh√¥ng c√≥ kh·ªëi u)"
            prediction = {"result": result, "probability": prob}

            flash(f"AI: {result} ({prob*100:.1f}%)", "success")

        except Exception as e:
            logger.error(f"Predict error: {e}")
            flash(f"‚ùå L·ªói AI: {e}", "danger")
        finally:
            if tmp_path and os.path.exists(tmp_path):
                os.remove(tmp_path)

    return render_template(
        "emr_prediction.html",
        prediction=prediction,
        filename=filename,
        image_b64=image_b64
    )

# === RUN ===
if __name__ == "__main__":
    port = int(os.environ.get("PORT", 5000))
    logger.info(f"EMR AI starting on port {port}")
    app.run(host="0.0.0.0", port=port, debug=False, threaded=False)
